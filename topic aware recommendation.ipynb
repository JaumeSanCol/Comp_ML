{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     5\n",
      "1     4\n",
      "2     3\n",
      "3     4\n",
      "4     5\n",
      "5     3\n",
      "6     5\n",
      "7     2\n",
      "8     4\n",
      "9     5\n",
      "10    3\n",
      "11    4\n",
      "12    5\n",
      "13    3\n",
      "14    4\n",
      "Name: rating, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bough\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bough\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# Download stopwords if not already present\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"small dataset.csv\")  # Replace with actual file path\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
    "    tokens = word_tokenize(text)  # Tokenize\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english') and len(word) > 2]  # Remove stopwords and short words\n",
    "    return ' '.join(tokens)\n",
    "print(df['rating'])\n",
    "\n",
    "# Apply preprocessing\n",
    "df[\"cleaned_review\"] = df[\"review\"].astype(str).apply(preprocess_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:  quality battery life short great sound cancellation noise headphones excellent\n",
      "Topic 1:  good deep learning technical bit introduction keeps warm winter jacket\n",
      "Topic 2:  written works design could better informative hard follow chapters well\n",
      "Topic 3:  recommend material fit songs feel beats repetitive comfortable durable fabric\n",
      "Topic 4:  songs every moment masterpiece loved predictable cinematography plot overall style\n"
     ]
    }
   ],
   "source": [
    "# Vectorize using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(df[\"cleaned_review\"])\n",
    "\n",
    "# Vectorize using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(df[\"cleaned_review\"])\n",
    "\n",
    "# Extract topics using LDA\n",
    "num_topics = 5  # Adjust based on dataset\n",
    "lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "item_topics = lda_model.fit_transform(X)  # Topic distribution per item\n",
    "\n",
    "df[\"topic_distribution\"] = list(item_topics)  # Store topic distributions\n",
    "\n",
    "\n",
    "# Display topics\n",
    "def display_topics(model, feature_names, num_words=10):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx}: \", \" \".join([feature_names[i] for i in topic.argsort()[-num_words:]]))\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "display_topics(lda_model, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   item_id  similarity\n",
      "1      102    0.886484\n",
      "0      101    0.885546\n",
      "3      104    0.671988\n",
      "2      103    0.577523\n",
      "4      105    0.410543\n"
     ]
    }
   ],
   "source": [
    "# Aggregate topic distributions per item (item profiles)\n",
    "item_profiles = df.groupby(\"item_id\")[\"topic_distribution\"].apply(lambda x: np.mean(np.vstack(x), axis=0)).reset_index()\n",
    "\n",
    "# Function to get user profile based on rated item profiles\n",
    "def get_user_profile(user_id):\n",
    "    user_data = df[df[\"user_id\"] == user_id].merge(item_profiles, on=\"item_id\", how=\"left\")\n",
    "    liked = user_data[user_data[\"rating\"] >= 4][\"topic_distribution_y\"]\n",
    "    disliked = user_data[user_data[\"rating\"] <= 2][\"topic_distribution_y\"]\n",
    "    \n",
    "    if not liked.empty:\n",
    "        liked_profile = np.mean(np.vstack(liked), axis=0)\n",
    "    else:\n",
    "        liked_profile = np.zeros(num_topics)\n",
    "    \n",
    "    if not disliked.empty:\n",
    "        disliked_profile = np.mean(np.vstack(disliked), axis=0)\n",
    "    else:\n",
    "        disliked_profile = np.zeros(num_topics)\n",
    "    \n",
    "    return liked_profile, disliked_profile\n",
    "\n",
    "# Function to recommend items based on item profiles\n",
    "def recommend_items(user_id, top_n=5):\n",
    "    liked_profile, disliked_profile = get_user_profile(user_id)\n",
    "    \n",
    "    item_profiles[\"similarity\"] = item_profiles[\"topic_distribution\"].apply(\n",
    "        lambda x: cosine_similarity([x], [liked_profile])[0][0] - cosine_similarity([x], [disliked_profile])[0][0]\n",
    "    )\n",
    "    \n",
    "    recommendations = item_profiles.sort_values(by=\"similarity\", ascending=False).head(top_n)\n",
    "    return recommendations[[\"item_id\", \"similarity\"]]\n",
    "\n",
    "# Example: Get recommendations for user 1\n",
    "user_id = 1\n",
    "print(recommend_items(user_id))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.29156684, 0.18039232, 0.40349133, 0.06226194, 0.06228757]), array([0.06516561, 0.06528794, 0.29540629, 0.0652751 , 0.50886507]))\n"
     ]
    }
   ],
   "source": [
    "print(get_user_profile(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
