{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_path = \"Yelp JSON/yelp_dataset/yelp_academic_dataset_review.json\"\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 100000  # Adjust based on available memory\n",
    "\n",
    "# Create an empty list to store dataframes\n",
    "dfs = []\n",
    "\n",
    "with open(review_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    chunk = []\n",
    "    for i, line in enumerate(f):\n",
    "        chunk.append(json.loads(line))\n",
    "        \n",
    "        # Process in chunks\n",
    "        if (i + 1) % chunk_size == 0:\n",
    "            df_chunk = pd.DataFrame(chunk)\n",
    "            dfs.append(df_chunk)  # Store in list to avoid memory explosion\n",
    "            chunk = []  # Reset chunk\n",
    "    \n",
    "    # Process remaining lines\n",
    "    if chunk:\n",
    "        df_chunk = pd.DataFrame(chunk)\n",
    "        dfs.append(df_chunk)\n",
    "\n",
    "# Concatenate all chunks into a single DataFrame\n",
    "reviews = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "businesses_path = \"Yelp JSON/yelp_dataset/yelp_academic_dataset_business.json\"\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 100000  # Adjust based on available memory\n",
    "\n",
    "# Create an empty list to store dataframes\n",
    "dfs = []\n",
    "\n",
    "with open(businesses_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    chunk = []\n",
    "    for i, line in enumerate(f):\n",
    "        chunk.append(json.loads(line))\n",
    "        \n",
    "        # Process in chunks\n",
    "        if (i + 1) % chunk_size == 0:\n",
    "            df_chunk = pd.DataFrame(chunk)\n",
    "            dfs.append(df_chunk)  # Store in list to avoid memory explosion\n",
    "            chunk = []  # Reset chunk\n",
    "    \n",
    "    # Process remaining lines\n",
    "    if chunk:\n",
    "        df_chunk = pd.DataFrame(chunk)\n",
    "        dfs.append(df_chunk)\n",
    "\n",
    "# Concatenate all chunks into a single DataFrame\n",
    "businesses = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>stars</th>\n",
       "      <th>review_count</th>\n",
       "      <th>is_open</th>\n",
       "      <th>attributes</th>\n",
       "      <th>categories</th>\n",
       "      <th>hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pns2l4eNsfO8kk83dixA6A</td>\n",
       "      <td>Abby Rappoport, LAC, CMQ</td>\n",
       "      <td>1616 Chapala St, Ste 2</td>\n",
       "      <td>Santa Barbara</td>\n",
       "      <td>CA</td>\n",
       "      <td>93101</td>\n",
       "      <td>34.426679</td>\n",
       "      <td>-119.711197</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>{'ByAppointmentOnly': 'True'}</td>\n",
       "      <td>Doctors, Traditional Chinese Medicine, Naturop...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mpf3x-BjTdTEA3yCZrAYPw</td>\n",
       "      <td>The UPS Store</td>\n",
       "      <td>87 Grasso Plaza Shopping Center</td>\n",
       "      <td>Affton</td>\n",
       "      <td>MO</td>\n",
       "      <td>63123</td>\n",
       "      <td>38.551126</td>\n",
       "      <td>-90.335695</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>{'BusinessAcceptsCreditCards': 'True'}</td>\n",
       "      <td>Shipping Centers, Local Services, Notaries, Ma...</td>\n",
       "      <td>{'Monday': '0:0-0:0', 'Tuesday': '8:0-18:30', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tUFrWirKiKi_TAnsVWINQQ</td>\n",
       "      <td>Target</td>\n",
       "      <td>5255 E Broadway Blvd</td>\n",
       "      <td>Tucson</td>\n",
       "      <td>AZ</td>\n",
       "      <td>85711</td>\n",
       "      <td>32.223236</td>\n",
       "      <td>-110.880452</td>\n",
       "      <td>3.5</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>{'BikeParking': 'True', 'BusinessAcceptsCredit...</td>\n",
       "      <td>Department Stores, Shopping, Fashion, Home &amp; G...</td>\n",
       "      <td>{'Monday': '8:0-22:0', 'Tuesday': '8:0-22:0', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MTSW4McQd7CbVtyjqoe9mw</td>\n",
       "      <td>St Honore Pastries</td>\n",
       "      <td>935 Race St</td>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>PA</td>\n",
       "      <td>19107</td>\n",
       "      <td>39.955505</td>\n",
       "      <td>-75.155564</td>\n",
       "      <td>4.0</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>{'RestaurantsDelivery': 'False', 'OutdoorSeati...</td>\n",
       "      <td>Restaurants, Food, Bubble Tea, Coffee &amp; Tea, B...</td>\n",
       "      <td>{'Monday': '7:0-20:0', 'Tuesday': '7:0-20:0', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mWMc6_wTdE0EUBKIGXDVfA</td>\n",
       "      <td>Perkiomen Valley Brewery</td>\n",
       "      <td>101 Walnut St</td>\n",
       "      <td>Green Lane</td>\n",
       "      <td>PA</td>\n",
       "      <td>18054</td>\n",
       "      <td>40.338183</td>\n",
       "      <td>-75.471659</td>\n",
       "      <td>4.5</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>{'BusinessAcceptsCreditCards': 'True', 'Wheelc...</td>\n",
       "      <td>Brewpubs, Breweries, Food</td>\n",
       "      <td>{'Wednesday': '14:0-22:0', 'Thursday': '16:0-2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                      name  \\\n",
       "0  Pns2l4eNsfO8kk83dixA6A  Abby Rappoport, LAC, CMQ   \n",
       "1  mpf3x-BjTdTEA3yCZrAYPw             The UPS Store   \n",
       "2  tUFrWirKiKi_TAnsVWINQQ                    Target   \n",
       "3  MTSW4McQd7CbVtyjqoe9mw        St Honore Pastries   \n",
       "4  mWMc6_wTdE0EUBKIGXDVfA  Perkiomen Valley Brewery   \n",
       "\n",
       "                           address           city state postal_code  \\\n",
       "0           1616 Chapala St, Ste 2  Santa Barbara    CA       93101   \n",
       "1  87 Grasso Plaza Shopping Center         Affton    MO       63123   \n",
       "2             5255 E Broadway Blvd         Tucson    AZ       85711   \n",
       "3                      935 Race St   Philadelphia    PA       19107   \n",
       "4                    101 Walnut St     Green Lane    PA       18054   \n",
       "\n",
       "    latitude   longitude  stars  review_count  is_open  \\\n",
       "0  34.426679 -119.711197    5.0             7        0   \n",
       "1  38.551126  -90.335695    3.0            15        1   \n",
       "2  32.223236 -110.880452    3.5            22        0   \n",
       "3  39.955505  -75.155564    4.0            80        1   \n",
       "4  40.338183  -75.471659    4.5            13        1   \n",
       "\n",
       "                                          attributes  \\\n",
       "0                      {'ByAppointmentOnly': 'True'}   \n",
       "1             {'BusinessAcceptsCreditCards': 'True'}   \n",
       "2  {'BikeParking': 'True', 'BusinessAcceptsCredit...   \n",
       "3  {'RestaurantsDelivery': 'False', 'OutdoorSeati...   \n",
       "4  {'BusinessAcceptsCreditCards': 'True', 'Wheelc...   \n",
       "\n",
       "                                          categories  \\\n",
       "0  Doctors, Traditional Chinese Medicine, Naturop...   \n",
       "1  Shipping Centers, Local Services, Notaries, Ma...   \n",
       "2  Department Stores, Shopping, Fashion, Home & G...   \n",
       "3  Restaurants, Food, Bubble Tea, Coffee & Tea, B...   \n",
       "4                          Brewpubs, Breweries, Food   \n",
       "\n",
       "                                               hours  \n",
       "0                                               None  \n",
       "1  {'Monday': '0:0-0:0', 'Tuesday': '8:0-18:30', ...  \n",
       "2  {'Monday': '8:0-22:0', 'Tuesday': '8:0-22:0', ...  \n",
       "3  {'Monday': '7:0-20:0', 'Tuesday': '7:0-20:0', ...  \n",
       "4  {'Wednesday': '14:0-22:0', 'Thursday': '16:0-2...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "businesses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "business_filtered = businesses[businesses[\"city\"] == \"Sparks\"]\n",
    "business_ids = business_filtered[\"business_id\"]\n",
    "filtered_reviews = reviews[reviews[\"business_id\"].isin(business_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filtered_reviews.drop(columns=[\"useful\", \"funny\", \"cool\",\"date\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_reviews.to_csv(\"filtered_reviews.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_reviews= pd.read_csv(\"filtered_reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove ratings to test with later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (67129, 5)\n",
      "Holdout shape: (5904, 5)\n"
     ]
    }
   ],
   "source": [
    "def create_holdout_dataset(df, test_users, test_size=0.2, min_ratings=5):\n",
    "    \"\"\"\n",
    "    Creates a holdout dataset by removing some reviews from test users, \n",
    "    ensuring that only users with enough reviews are considered.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Original dataframe (train)\n",
    "        test_users (list): List of user IDs to include in test set\n",
    "        test_size (float): Proportion of reviews to remove\n",
    "        min_ratings (int): Minimum number of reviews required per user\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Training dataset with removed items\n",
    "        pd.DataFrame: Holdout dataset with removed items\n",
    "    \"\"\"\n",
    "    # Filter test users with sufficient total reviews\n",
    "    eligible_users = df.groupby('user_id').filter(lambda x: len(x) >= min_ratings)['user_id'].unique()\n",
    "\n",
    "    # Create holdout set\n",
    "    holdout = []\n",
    "    train = df.copy()\n",
    "    \n",
    "    for user in eligible_users:\n",
    "        # Get all reviews of the user\n",
    "        user_reviews = df[df['user_id'] == user]\n",
    "        \n",
    "        # Randomly select items to remove\n",
    "        removed = user_reviews.sample(frac=test_size)\n",
    "        holdout.append(removed)\n",
    "        \n",
    "        # Remove from training data\n",
    "        train = train.drop(removed.index)\n",
    "\n",
    "    return train, pd.concat(holdout)\n",
    "\n",
    "# Apply the function\n",
    "train, holdout = create_holdout_dataset(filtered_reviews, test_users=filtered_reviews['user_id'].unique(), test_size=0.2, min_ratings=5)\n",
    "\n",
    "# Print dataset shapes\n",
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Holdout shape:\", holdout.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('place', 31592), ('time', 31001), ('food', 30299), ('get', 27851), ('good', 27796), ('servic', 26550), ('great', 26345), ('go', 26210), ('order', 24529), ('like', 22617), ('one', 21944), ('back', 21161), ('would', 20417), ('us', 15333), ('tri', 15246), ('come', 14812), ('even', 14231), ('work', 14173), ('got', 14144), ('call', 14077), ('realli', 13368), ('love', 12962), ('look', 12938), ('also', 12479), ('make', 12469), ('alway', 12415), ('want', 12366), ('dont', 12312), ('wait', 12198), ('day', 12144), ('need', 12106), ('ask', 12015), ('price', 11709), ('never', 11509), ('custom', 11497), ('well', 11275), ('staff', 11200), ('nice', 11110), ('best', 10991), ('friendli', 10927), ('didnt', 10847), ('im', 10538), ('went', 10471), ('came', 10467), ('said', 10467), ('could', 10050), ('take', 9955), ('recommend', 9760), ('first', 9725), ('know', 9552), ('busi', 9478), ('peopl', 9349), ('ive', 9340), ('help', 9232), ('say', 9182), ('new', 9114), ('use', 9019), ('told', 8990), ('year', 8760), ('restaur', 8543), ('clean', 8325), ('give', 8299), ('minut', 8278), ('made', 8249), ('experi', 8158), ('pizza', 8071), ('thing', 7885), ('littl', 7834), ('eat', 7811), ('amaz', 7780), ('way', 7779), ('much', 7773), ('two', 7723), ('definit', 7630), ('right', 7543), ('took', 7311), ('locat', 7309), ('chicken', 7140), ('see', 7124), ('still', 6912), ('room', 6908), ('hour', 6885), ('store', 6808), ('ever', 6784), ('check', 6716), ('2', 6677), ('sure', 6660), ('area', 6638), ('reno', 6592), ('fri', 6565), ('better', 6563), ('everyth', 6468), ('home', 6465), ('lot', 6432), ('star', 6404), ('manag', 6394), ('review', 6357), ('delici', 6350), ('everi', 6324), ('tabl', 6292)]\n"
     ]
    }
   ],
   "source": [
    "# Ensure required resources are downloaded\n",
    "\"\"\" nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet') \"\"\"\n",
    "\n",
    "# Load NLTK stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize stemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Function to clean text, remove stopwords, apply lemmatization and stemming\n",
    "def preprocess_text(text):\n",
    "    text = text.lower().translate(str.maketrans('', '', string.punctuation))  # Lowercase and remove punctuation\n",
    "    tokens = word_tokenize(text)  # Tokenization\n",
    "    words = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
    "    \n",
    "    words = [stemmer.stem(word) for word in words]  # Stemming\n",
    "    return ' '.join(words)  # Convert list back to string\n",
    "\n",
    "# Apply preprocessing directly to the 'text' column\n",
    "train['text'] = train['text'].astype(str).apply(preprocess_text)\n",
    "\n",
    "# Count most common words\n",
    "word_list = ' '.join(train['text']).split()\n",
    "word_counts = Counter(word_list)\n",
    "most_common_words = word_counts.most_common(100)\n",
    "\n",
    "# Display the most common words\n",
    "print(most_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['place', 'time', 'food', 'get', 'good', 'servic', 'go', 'great', 'order', 'like', 'one', 'back', 'would', 'us', 'tri', 'come', 'even', 'got', 'call', 'work', 'realli', 'look', 'love', 'want', 'dont', 'make', 'alway', 'also', 'wait', 'ask', 'need', 'day', 'price', 'never', 'custom', 'well', 'staff', 'nice', 'didnt', 'best', 'friendli', 'said', 'im', 'went', 'came', 'could', 'take', 'first', 'recommend', 'know', 'busi', 'peopl', 'ive', 'say', 'help', 'told', 'new', 'use', 'year', 'restaur', 'minut', 'give', 'clean', 'made', 'pizza', 'experi', 'thing', 'way', 'eat', 'much', 'littl', 'two', 'amaz', 'right', 'definit', 'took', 'locat', 'chicken', 'see', 'still', 'store', 'hour', 'ever', '2', 'room', 'check', 'sure', 'better', 'reno', 'area', 'fri', 'manag', 'everyth', 'star', 'home', 'review', 'tabl', 'lot', 'anoth', 'everi']\n"
     ]
    }
   ],
   "source": [
    "print([word[0] for word in most_common_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "irrelevant_words =['place', 'time', 'good', 'get', 'go', 'great', 'order', 'like', 'one', 'back', 'would', 'tri', 'us', 'come', 'got', 'even', 'work', 'call', 'realli', 'love', 'look', 'alway', 'also', 'make', 'want', 'dont', 'wait', 'need', 'day', 'ask', 'price', 'never', 'well', 'custom', 'nice',  'best', 'didnt', 'im', 'went', 'came', 'said', 'take', 'could', 'first', 'know', 'recommend', 'busi', 'ive', 'peopl', 'say', 'help', 'new', 'use', 'told', 'year',  'give', 'minut', 'made', 'littl', 'thing', 'much', 'way', 'two', 'amaz', 'right', 'definit', 'locat', 'took', 'see', 'still', 'hour', 'fri', '2', 'check', 'ever', 'sure', 'area', 'better', 'lot', 'everyth', 'star', 'reno', 'home', 'tabl', 'review', 'everi', 'manag'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_irrelevant_words(text, irrelevant_words):\n",
    "    \"\"\"\n",
    "    Remove irrelevant words from a text string.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        irrelevant_words (list): List of words to remove\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned text with irrelevant words removed\n",
    "    \"\"\"\n",
    "    # Create a regex pattern to match whole words only\n",
    "    pattern = r'\\b(?:{})\\b'.format('|'.join(map(re.escape, irrelevant_words)))\n",
    "    \n",
    "    # Remove the words and clean up extra spaces\n",
    "    cleaned_text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "train['cleaned_text'] = train['text'].apply(lambda x: remove_irrelevant_words(x, irrelevant_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOPIC AWARE RECOMMENDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(train):\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    X = vectorizer.fit_transform(train[\"cleaned_text\"])\n",
    "    return vectorizer, X\n",
    "\n",
    "def train_lda(X, num_topics):\n",
    "    lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42,doc_topic_prior=0.01, topic_word_prior=0.01)\n",
    "    item_topics = lda_model.fit_transform(X)\n",
    "    return lda_model, item_topics\n",
    "\n",
    "def create_item_profiles(train, item_topics):\n",
    "    train[\"topic_distribution\"] = list(item_topics)\n",
    "    item_profiles = train.groupby(\"business_id\")[\"topic_distribution\"].apply(lambda x: np.mean(np.vstack(x), axis=0)).reset_index()\n",
    "    return item_profiles\n",
    "\n",
    "def get_user_profile(user_id, train, item_profiles, num_topics):\n",
    "\n",
    "    user_data = train[train[\"user_id\"] == user_id].merge(item_profiles, on=\"business_id\", how=\"left\")\n",
    "    liked = user_data[user_data[\"stars\"] >= 1][\"topic_distribution_y\"]\n",
    "    disliked = user_data[user_data[\"stars\"] <= 2][\"topic_distribution_y\"]\n",
    "    liked_profile = np.mean(np.vstack(liked), axis=0) if not liked.empty else np.zeros(num_topics)\n",
    "    disliked_profile = np.mean(np.vstack(disliked), axis=0) if not disliked.empty else np.zeros(num_topics)\n",
    "    return liked_profile, disliked_profile\n",
    "\n",
    "def recommend_items_by_topic(user_id, train, item_profiles, num_topics):\n",
    "    \"\"\"assigns scores to businesses and returns a dictionary of business_id and score in range [0,1]\"\"\"\n",
    "    liked_profile, disliked_profile = get_user_profile(user_id, train, item_profiles, num_topics)\n",
    "    rated_businesses = train[train['user_id'] == user_id]['business_id'].unique()\n",
    "    unrated_items = item_profiles[~item_profiles['business_id'].isin(rated_businesses)].copy()\n",
    "    unrated_items[\"similarity\"] = unrated_items[\"topic_distribution\"].apply(\n",
    "        lambda x: cosine_similarity([x], [liked_profile])[0][0] -0*cosine_similarity([x], [disliked_profile])[0][0]\n",
    "    )\n",
    "\n",
    "    return dict(zip(unrated_items['business_id'], unrated_items['similarity']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from surprise import Dataset, Reader, SVD,KNNBasic,accuracy\n",
    "from surprise.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svd_model(train,factors=5):\n",
    "    \"\"\"Trains an SVD model on the given dataset.\"\"\"\n",
    "    # Define the rating scale (e.g., 1-5 stars)\n",
    "    reader = Reader(rating_scale=(1, 5))\n",
    "    \n",
    "    # Load data into Surprise's Dataset format\n",
    "    data = Dataset.load_from_df(\n",
    "        train[[\"user_id\", \"business_id\", \"stars\"]], \n",
    "        reader\n",
    "    )\n",
    "    # Split into train/test (optional, can use full data)\n",
    "    trainset = data.build_full_trainset()\n",
    "    \n",
    "    # Initialize and train SVD\n",
    "    model = SVD(n_factors=factors, n_epochs=20, lr_all=0.005, reg_all=0.02)\n",
    "    model.fit(trainset)\n",
    "    \n",
    "    return model\n",
    "def train_cbf_model(train):\n",
    "    \"\"\"Trains an SVD model on the given dataset.\"\"\"\n",
    "    # Define the rating scale (e.g., 1-5 stars)\n",
    "    reader = Reader(rating_scale=(1, 5))\n",
    "    \n",
    "    # Load data into Surprise's Dataset format\n",
    "    data = Dataset.load_from_df(\n",
    "        train[[\"user_id\", \"business_id\", \"stars\"]], \n",
    "        reader\n",
    "    )\n",
    "    # Split into train/test (optional, can use full data)\n",
    "    trainset = data.build_full_trainset()\n",
    "    \n",
    "    # Initialize and train ibcf model\n",
    "    model = KNNBasic(sim_options={'user_based': False})\n",
    "    model.fit(trainset)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_for_user_with_model(model, user_id, business_ids):\n",
    "    \"\"\"Generates recommendations for a user.\"\"\"\n",
    "    # Predict ratings for all businesses the user hasn't rated\n",
    "    predictions = {}\n",
    "    for biz_id in business_ids:\n",
    "        pred = model.predict(user_id, biz_id)\n",
    "        predictions[biz_id]= pred.est  # (business_id, predicted_rating)\n",
    "    \n",
    "\n",
    "    \n",
    "   \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unrated_businesses(train_data, user_id):\n",
    "    rated = train_data[train_data[\"user_id\"] == user_id][\"business_id\"].unique()\n",
    "    all_businesses = train_data[\"business_id\"].unique()\n",
    "    return list(set(all_businesses) - set(rated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_recommendation(user_id, train,num_topics,item_profiles,model1, model2, w1=1, w2=1, w3=1):\n",
    "    topic_recommendations = recommend_items_by_topic(user_id, train, item_profiles, num_topics)\n",
    "    unrated_businesses = get_unrated_businesses(train, user_id)\n",
    "    svd_recommendations = recommend_for_user_with_model(model1, user_id, unrated_businesses)\n",
    "    ibcf_recommendations = recommend_for_user_with_model(model2, user_id, unrated_businesses)\n",
    "    \n",
    "    hybrid_scores = {}\n",
    "    for biz_id in set(topic_recommendations.keys()) | set(svd_recommendations.keys()) | set(ibcf_recommendations.keys()):\n",
    "        topic_score = topic_recommendations.get(biz_id, 0)\n",
    "        svd_score = svd_recommendations.get(biz_id, 0)\n",
    "        ibcf_score = ibcf_recommendations.get(biz_id, 0)\n",
    "        hybrid_scores[biz_id] = (w1 * 5*topic_score + w2 * svd_score + w3 * ibcf_score)/(w1 + w2 + w3)\n",
    "    \n",
    "\n",
    "     \n",
    "    \n",
    "    return hybrid_scores\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer, X = vectorize_text(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Topics: 50, Perplexity: 14027.33701329771\n",
      "Number of Topics: 60, Perplexity: 17525.938810917374\n",
      "Number of Topics: 70, Perplexity: 23028.35403650486\n",
      "Number of Topics: 80, Perplexity: 28612.638244991733\n",
      "Number of Topics: 90, Perplexity: 36083.66361096018\n"
     ]
    }
   ],
   "source": [
    "def compute_perplexity_for_topics(X, topic_range, vectorizer=None):\n",
    "\n",
    "    perplexities = {}\n",
    "    for num_topics in topic_range:\n",
    "        # Train LDA model\n",
    "        lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "        lda.fit(X)\n",
    "        \n",
    "        # Compute perplexity\n",
    "        perplexity = lda.perplexity(X)\n",
    "        perplexities[num_topics] = perplexity\n",
    "        print(f\"Number of Topics: {num_topics}, Perplexity: {perplexity}\")\n",
    "    \n",
    "    return perplexities\n",
    "\n",
    "\n",
    "topic_range = range(50, 100, 10)  # Try topic numbers from 5 to 20\n",
    "perplexities = compute_perplexity_for_topics(X, topic_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 5 # Number of topics for LDA\n",
    "\n",
    "lda_model, item_topics = train_lda(X, num_topics)\n",
    "item_profiles = create_item_profiles(train, item_topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Topic 1: food, servic, chicken, delici, restaur, breakfast, friendli, fresh, flavor, eat\n",
      "\n",
      "ðŸ”¹ Topic 2: car, servic, job, compani, done, room, nail, clean, guy, thank\n",
      "\n",
      "ðŸ”¹ Topic 3: store, staff, friendli, servic, clean, shop, dr, thank, food, experi\n",
      "\n",
      "ðŸ”¹ Topic 4: food, servic, sushi, rude, drink, employe, eat, seat, bad, walk\n",
      "\n",
      "ðŸ”¹ Topic 5: pizza, food, beer, servic, bar, sandwich, salad, friendli, drink, staff\n"
     ]
    }
   ],
   "source": [
    "# Function to display topics\n",
    "def display_topics(lda, vectorizer, top_words=10):\n",
    "    words = np.array(vectorizer.get_feature_names_out())\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_features = topic.argsort()[-top_words:][::-1]  # Top words for the topic\n",
    "        print(f\"\\nðŸ”¹ Topic {topic_idx + 1}: \" + \", \".join(words[top_features]))\n",
    "display_topics(lda_model, vectorizer, top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    }
   ],
   "source": [
    "model1= train_svd_model(train,factors=5)\n",
    "\n",
    "model2= train_cbf_model(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1801,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holdout['user_id'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_hybrid_model(train, holdout, item_profiles, num_topics,model1, model2,w1=1, w2=1, w3=1):\n",
    "    errors = []\n",
    "    for user_id in holdout[\"user_id\"].unique():\n",
    "        recommendations = hybrid_recommendation(user_id, train,num_topics,item_profiles, model1, model2, w1, w2, w3)\n",
    "        actual_ratings = holdout[holdout[\"user_id\"] == user_id].set_index(\"business_id\")[\"stars\"]\n",
    "        predicted_ratings = pd.Series(recommendations).reindex(actual_ratings.index).fillna(3)  # Default to neutral rating\n",
    "        \n",
    "        errors.extend(np.abs(actual_ratings - predicted_ratings))\n",
    "    \n",
    "    mae = np.mean(errors)\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5667392267744003"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_hybrid_model(train, holdout.iloc[:100], item_profiles,num_topics, model1, model2,w1=1, w2=0, w3=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
